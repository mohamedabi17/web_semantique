#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Projet Master 2 - Web S√©mantique : Extraction de Graphes de Connaissances a large √©chelle
Sujet 1 : Architecture T-Box/A-Box avec R√©ification

Ce script d√©montre une impl√©mentation acad√©mique rigoureuse de :
- D√©finition d'ontologie (T-Box) avec OWL
- Extraction d'entit√©s et instanciation (A-Box)
- R√©ification RDF pour les m√©tadonn√©es
"""

import re
import unicodedata
import json
import os
import sys
import requests
import time
from groq import Groq
from huggingface_hub import InferenceClient
from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, BNode
from rdflib.namespace import XSD, DC, FOAF
import spacy
import networkx as nx
import matplotlib.pyplot as plt
from typing import Optional
from dotenv import load_dotenv

# Chargement des variables d'environnement depuis .env
load_dotenv()


# ============================================================================
# 1. CONFIGURATION ET NAMESPACES
# ============================================================================

# Namespace personnalis√© pour notre ontologie de domaine
EX = Namespace("http://example.org/master2/ontology#")

# Namespace pour les instances (donn√©es)
DATA = Namespace("http://example.org/master2/data#")

# Namespace Schema.org pour types standards
SCHEMA = Namespace("http://schema.org/")

# ============================================================================
# CONFIGURATION DE L'API HUGGING FACE (VRAIE IMPL√âMENTATION LLM) ‚≠ê
# ============================================================================

# -------------------------------------------------------------------------
# API Hugging Face Inference - Mod√®le gratuit et accessible
# Utilisation de Qwen2.5-Coder-32B-Instruct (gratuit et performant pour extraction d'informations)
# -------------------------------------------------------------------------
API_URL = "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-Coder-32B-Instruct"
# Note: Si ce mod√®le ne fonctionne pas, le fallback "relatedTo" prendra le relais
# -------------------------------------------------------------------------

# Token Hugging Face (charg√© depuis .env)
HF_TOKEN = os.getenv("HF_TOKEN", "")
if not HF_TOKEN:
    print("‚ö†Ô∏è ATTENTION : HF_TOKEN non trouv√©. Cr√©ez un fichier .env avec votre token.")

# Headers pour l'authentification
HEADERS = {"Authorization": f"Bearer {HF_TOKEN}"}


# ============================================================================
# 2. D√âFINITION DE L'ONTOLOGIE (T-BOX) - SCH√âMA CONCEPTUEL
# ============================================================================

def define_tbox(graph):
    """
    D√©finit le sch√©ma conceptuel de l'ontologie selon les standards OWL.
    
    Cette fonction cr√©e la T-Box (Terminological Box) qui contient :
    - Les d√©clarations de classes (owl:Class)
    - Les propri√©t√©s d'objet (owl:ObjectProperty) 
    - Les propri√©t√©s de type de donn√©es (owl:DatatypeProperty)
    - Les contraintes de domaine (rdfs:domain) et de port√©e (rdfs:range)
    
    Args:
        graph (rdflib.Graph): Le graphe RDF o√π ajouter les d√©clarations
    """
    
    print("\n[T-BOX] D√©finition de l'ontologie...")
    
    # -----------------------------------------------------------------------
    # 2.1 D√âCLARATION DES CLASSES (owl:Class)
    # -----------------------------------------------------------------------
    # Utilisation d'ontologies standards : FOAF et Schema.org
    # Ceci am√©liore l'interop√©rabilit√© et la r√©utilisabilit√© du graphe
    
    # FOAF:Person - Standard du Web S√©mantique pour les personnes
    # (Friend of a Friend - ontologie largement adopt√©e)
    graph.add((FOAF.Person, RDF.type, OWL.Class))
    graph.add((FOAF.Person, RDFS.label, Literal("Personne", lang="fr")))
    graph.add((FOAF.Person, RDFS.comment, 
               Literal("Classe FOAF repr√©sentant un individu humain", lang="fr")))
    
    # Schema:Place - Standard Schema.org pour les lieux
    # (Schema.org - ontologie de r√©f√©rence pour les moteurs de recherche)
    graph.add((SCHEMA.Place, RDF.type, OWL.Class))
    graph.add((SCHEMA.Place, RDFS.label, Literal("Lieu", lang="fr")))
    graph.add((SCHEMA.Place, RDFS.comment, 
               Literal("Classe Schema.org repr√©sentant un lieu g√©ographique ou une institution", lang="fr")))
    
    # Schema:Organization - Standard Schema.org pour les organisations
    graph.add((SCHEMA.Organization, RDF.type, OWL.Class))
    graph.add((SCHEMA.Organization, RDFS.label, Literal("Organisation", lang="fr")))
    graph.add((SCHEMA.Organization, RDFS.comment, 
               Literal("Classe Schema.org repr√©sentant une entit√© organisationnelle", lang="fr")))
    
    # Classe repr√©sentant un Document/Cours
    graph.add((EX.Document, RDF.type, OWL.Class))
    graph.add((EX.Document, RDFS.label, Literal("Document", lang="fr")))
    graph.add((EX.Document, RDFS.comment, 
               Literal("Classe repr√©sentant un document, cours ou publication", lang="fr")))
    
    # -----------------------------------------------------------------------
    # 2.1.1 CLASSE AVEC RESTRICTION OWL (DIFF√âRENCIE OWL DE RDFS) ‚≠ê
    # -----------------------------------------------------------------------
    # POINT CL√â ACAD√âMIQUE : Ceci d√©montre l'utilisation d'OWL au-del√† de RDFS
    # Une restriction OWL permet de d√©finir des contraintes sur les propri√©t√©s
    
    # D√©claration de la sous-classe ValidatedCourse
    graph.add((EX.ValidatedCourse, RDF.type, OWL.Class))
    graph.add((EX.ValidatedCourse, RDFS.subClassOf, EX.Document))
    graph.add((EX.ValidatedCourse, RDFS.label, Literal("Cours Valid√©", lang="fr")))
    graph.add((EX.ValidatedCourse, RDFS.comment,
               Literal("Cours ayant au moins un auteur identifi√© (contrainte OWL)", lang="fr")))
    
    # Cr√©ation de la RESTRICTION OWL avec un Blank Node
    # Cette restriction stipule : "Un ValidatedCourse DOIT avoir au moins un ex:author qui est une foaf:Person"
    restriction = BNode()  # N≈ìud anonyme pour la restriction
    
    # La restriction est un owl:Restriction
    graph.add((restriction, RDF.type, OWL.Restriction))
    
    # La restriction porte sur la propri√©t√© ex:author
    graph.add((restriction, OWL.onProperty, EX.author))
    
    # La restriction exige : "il existe au moins une valeur de type foaf:Person"
    # owl:someValuesFrom = "some values from" (au moins une valeur provenant de...)
    graph.add((restriction, OWL.someValuesFrom, FOAF.Person))
    
    # Liaison de la restriction √† la classe ValidatedCourse
    graph.add((EX.ValidatedCourse, RDFS.subClassOf, restriction))
    
    print("  ‚úì Classe avec Restriction OWL cr√©√©e : ex:ValidatedCourse")
    print("    ‚Üí Contrainte : DOIT avoir au moins un ex:author de type foaf:Person")
    
    print("  ‚úì Classes standards utilis√©es : foaf:Person, schema:Place, schema:Organization, ex:Document, ex:ValidatedCourse")
    
    # -----------------------------------------------------------------------
    # 2.2 PROPRI√âT√âS D'OBJET (owl:ObjectProperty)
    # -----------------------------------------------------------------------
    # Ces propri√©t√©s lient des RESSOURCES entre elles (URI vers URI)
    # Elles respectent la contrainte : domain ‚Üí ObjectProperty ‚Üí range
    
    # Propri√©t√© : Une personne enseigne dans un lieu
    graph.add((EX.teaches, RDF.type, OWL.ObjectProperty))
    graph.add((EX.teaches, RDFS.label, Literal("enseigne √†", lang="fr")))
    graph.add((EX.teaches, RDFS.domain, FOAF.Person))  # Seules les Personnes peuvent enseigner
    graph.add((EX.teaches, RDFS.range, SCHEMA.Place))  # Elles enseignent dans des Lieux
    graph.add((EX.teaches, RDFS.comment, 
               Literal("Relation entre une personne et le lieu o√π elle enseigne", lang="fr")))
    
    # Propri√©t√© : Une personne r√©dige un document
    graph.add((EX.author, RDF.type, OWL.ObjectProperty))
    graph.add((EX.author, RDFS.label, Literal("a r√©dig√©", lang="fr")))
    graph.add((EX.author, RDFS.domain, FOAF.Person))  # Seules les Personnes r√©digent
    graph.add((EX.author, RDFS.range, EX.Document))  # Elles r√©digent des Documents
    graph.add((EX.a_redige, RDFS.comment, 
               Literal("Relation entre un auteur et un document qu'il a r√©dig√©", lang="fr")))
    
    # Propri√©t√© : Un document traite d'un sujet (repr√©sent√© par un concept)
    graph.add((EX.traite_de, RDF.type, OWL.ObjectProperty))
    graph.add((EX.traite_de, RDFS.label, Literal("traite de", lang="fr")))
    graph.add((EX.traite_de, RDFS.domain, EX.Document))  # Un Document traite de...
    graph.add((EX.traite_de, RDFS.range, RDFS.Resource))  # ...n'importe quelle ressource
    graph.add((EX.traite_de, RDFS.comment, 
               Literal("Relation entre un document et son sujet principal", lang="fr")))
    
    # Propri√©t√© : Une personne enseigne une mati√®re/sujet (nouveau : teaches peut pointer vers un TOPIC)
    graph.add((EX.teachesSubject, RDF.type, OWL.ObjectProperty))
    graph.add((EX.teachesSubject, RDFS.label, Literal("enseigne la mati√®re", lang="fr")))
    graph.add((EX.teachesSubject, RDFS.domain, FOAF.Person))  # Personne enseigne
    graph.add((EX.teachesSubject, RDFS.range, EX.Document))   # Une mati√®re (Topic/Document)
    graph.add((EX.teachesSubject, RDFS.comment, 
               Literal("Relation entre un enseignant et la mati√®re qu'il enseigne", lang="fr")))
    
    # Propri√©t√© : Une personne travaille dans une organisation
    graph.add((EX.worksAt, RDF.type, OWL.ObjectProperty))
    graph.add((EX.worksAt, RDFS.label, Literal("travaille √†", lang="fr")))
    graph.add((EX.worksAt, RDFS.domain, FOAF.Person))  # Seules les Personnes travaillent
    graph.add((EX.worksAt, RDFS.range, SCHEMA.Organization))  # Dans des Organisations
    graph.add((EX.worksAt, RDFS.comment, 
               Literal("Relation entre une personne et son lieu de travail", lang="fr")))
    
    # Propri√©t√© : Une organisation ou une personne est situ√©e dans un lieu
    graph.add((EX.locatedIn, RDF.type, OWL.ObjectProperty))
    graph.add((EX.locatedIn, RDFS.label, Literal("situ√© √†", lang="fr")))
    graph.add((EX.locatedIn, RDFS.domain, RDFS.Resource))  # Organisation ou Personne
    graph.add((EX.locatedIn, RDFS.range, SCHEMA.Place))  # Dans un Lieu
    graph.add((EX.locatedIn, RDFS.comment, 
               Literal("Relation de localisation g√©ographique", lang="fr")))
    
    # Propri√©t√© : Deux personnes collaborent ensemble
    graph.add((EX.collaboratesWith, RDF.type, OWL.ObjectProperty))
    graph.add((EX.collaboratesWith, RDFS.label, Literal("collabore avec", lang="fr")))
    graph.add((EX.collaboratesWith, RDFS.domain, FOAF.Person))
    graph.add((EX.collaboratesWith, RDFS.range, FOAF.Person))
    graph.add((EX.collaboratesWith, RDFS.comment, 
               Literal("Relation de collaboration entre deux personnes", lang="fr")))
    
    # Propri√©t√© : Une personne √©tudie dans une organisation
    graph.add((EX.studiesAt, RDF.type, OWL.ObjectProperty))
    graph.add((EX.studiesAt, RDFS.label, Literal("√©tudie √†", lang="fr")))
    graph.add((EX.studiesAt, RDFS.domain, FOAF.Person))
    graph.add((EX.studiesAt, RDFS.range, SCHEMA.Organization))
    graph.add((EX.studiesAt, RDFS.comment, 
               Literal("Relation entre un √©tudiant et son √©tablissement", lang="fr")))
    
    # Propri√©t√© : Une personne g√®re une organisation
    graph.add((EX.manages, RDF.type, OWL.ObjectProperty))
    graph.add((EX.manages, RDFS.label, Literal("g√®re", lang="fr")))
    graph.add((EX.manages, RDFS.domain, FOAF.Person))
    graph.add((EX.manages, RDFS.range, SCHEMA.Organization))
    graph.add((EX.manages, RDFS.comment, 
               Literal("Relation de gestion/direction d'une organisation", lang="fr")))
    
    # Propri√©t√© g√©n√©rique : Relation quelconque entre deux ressources
    graph.add((EX.relatedTo, RDF.type, OWL.ObjectProperty))
    graph.add((EX.relatedTo, RDFS.label, Literal("en relation avec", lang="fr")))
    graph.add((EX.relatedTo, RDFS.domain, RDFS.Resource))
    graph.add((EX.relatedTo, RDFS.range, RDFS.Resource))
    graph.add((EX.relatedTo, RDFS.comment, 
               Literal("Relation g√©n√©rique entre deux ressources", lang="fr")))
    
    print("  ‚úì ObjectProperties d√©finies : ex:teaches, ex:teachesSubject, ex:author, ex:traite_de, ex:worksAt, ex:locatedIn, ex:collaboratesWith, ex:studiesAt, ex:manages, ex:relatedTo")
    
    # -----------------------------------------------------------------------
    # 2.3 PROPRI√âT√âS DE DONN√âES (owl:DatatypeProperty)
    # -----------------------------------------------------------------------
    # Ces propri√©t√©s lient des RESSOURCES √† des VALEURS LITT√âRALES
    # Elles respectent la contrainte : domain ‚Üí DatatypeProperty ‚Üí xsd:datatype
    
    # Propri√©t√© : Le nom d'une personne (cha√Æne de caract√®res)
    # Utilisation de FOAF.name (propri√©t√© standard)
    graph.add((FOAF.name, RDF.type, OWL.DatatypeProperty))
    graph.add((FOAF.name, RDFS.label, Literal("nom", lang="fr")))
    graph.add((FOAF.name, RDFS.domain, FOAF.Person))  # S'applique aux Personnes
    graph.add((EX.nom, RDFS.range, XSD.string))  # Valeur de type cha√Æne
    graph.add((EX.nom, RDFS.comment, 
               Literal("Nom complet d'une personne", lang="fr")))
    
    # Propri√©t√© : L'intitul√© d'un document (cha√Æne de caract√®res)
    graph.add((EX.intitule, RDF.type, OWL.DatatypeProperty))
    graph.add((EX.intitule, RDFS.label, Literal("intitul√©", lang="fr")))
    graph.add((EX.intitule, RDFS.domain, EX.Document))  # S'applique aux Documents
    graph.add((EX.intitule, RDFS.range, XSD.string))  # Valeur de type cha√Æne
    graph.add((EX.intitule, RDFS.comment, 
               Literal("Titre ou intitul√© d'un document", lang="fr")))
    
    # Propri√©t√© : L'√¢ge d'une personne (entier)
    graph.add((EX.age, RDF.type, OWL.DatatypeProperty))
    graph.add((EX.age, RDFS.label, Literal("√¢ge", lang="fr")))
    graph.add((EX.age, RDFS.domain, EX.Person))  # S'applique aux Personnes
    graph.add((EX.age, RDFS.range, XSD.integer))  # Valeur de type entier
    graph.add((EX.age, RDFS.comment, 
               Literal("√Çge d'une personne en ann√©es", lang="fr")))
    
    print("  ‚úì DatatypeProperties d√©finies : foaf:name, ex:intitule, ex:age")
    print("[T-BOX] Ontologie d√©finie avec succ√®s !\n")


# ============================================================================
# 3. UTILITAIRES : NORMALISATION DES URIs
# ============================================================================

def normalize_uri_fragment(text):
    """
    Nettoie et normalise une cha√Æne de caract√®res pour cr√©er un fragment d'URI valide.
    
    Transformations appliqu√©es :
    - Suppression des accents (√© ‚Üí e, √† ‚Üí a, etc.)
    - Conversion en minuscules
    - Remplacement des espaces par des underscores
    - Conservation uniquement des caract√®res alphanum√©riques et underscores
    
    Args:
        text (str): Le texte √† normaliser
        
    Returns:
        str: Fragment d'URI normalis√© (ex: "Universit√© de Versailles" ‚Üí "universite_de_versailles")
    
    Exemple:
        >>> normalize_uri_fragment("Zoubida Kedad")
        'zoubida_kedad'
    """
    # D√©composition Unicode pour s√©parer les caract√®res de base des accents
    text = unicodedata.normalize('NFD', text)
    # Suppression des marques diacritiques (accents)
    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')
    # Normalisation : minuscules, espaces ‚Üí underscores, nettoyage
    text = text.lower().strip()
    text = re.sub(r'\s+', '_', text)  # Espaces multiples ‚Üí underscore unique
    text = re.sub(r'[^\w]', '', text)  # Suppression des caract√®res non alphanum√©riques
    return text


# ============================================================================
# 4. PR√âDICTION DE RELATIONS VIA LLM (SIMULATION)
# ============================================================================

def predict_relation_real_api(entity1: str, entity2: str, sentence: str) -> Optional[str]:
    """
    Utilise l'API GROQ (Gratuite et Ultra-Rapide).
    Mod√®le : Llama-3-8B (tr√®s performant pour l'extraction de relations).
    
    VERSION PRODUCTION (GROQ API) üöÄ :
    - API Groq gratuite et ultra-rapide
    - Mod√®le Meta Llama-3-8B-8192 (excellent pour le NLP)
    - Temp√©rature = 0 pour des r√©ponses stables et d√©terministes
    - Fallback intelligent si l'API est indisponible
    
    Args:
        entity1 (str): Premi√®re entit√© (g√©n√©ralement le sujet)
        entity2 (str): Deuxi√®me entit√© (g√©n√©ralement l'objet)
        sentence (str): Phrase compl√®te contenant les entit√©s
        
    Returns:
        str: Le type de relation d√©tect√© ("teaches", "author", "worksAt", "relatedTo")
    
    Exemples:
        >>> predict_relation_real_api("Marie", "Universit√©", "Marie enseigne √† l'Universit√©")
        'teaches'
    """
    
    # Cl√© API Groq (charg√©e depuis .env)
    GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
    if not GROQ_API_KEY:
        print("‚ö†Ô∏è ATTENTION : GROQ_API_KEY non trouv√©. Cr√©ez un fichier .env avec votre cl√©.")
        return "relatedTo"
    
    try:
        print(f"  üöÄ Appel API Groq (Llama-3) pour : {entity1} ‚Üî {entity2}")
        
        client = Groq(api_key=GROQ_API_KEY)
        
        # Prompt optimis√© pour Llama 3
        prompt = f"""Context: "{sentence}"
Analyze the relationship between the entities: "{entity1}" and "{entity2}".

You must choose ONE relation from this exact list:
1. teaches (if a PERSON teaches at a PLACE or ORGANIZATION)
2. teachesSubject (if a PERSON teaches a SUBJECT/TOPIC like Physics, Math, Computer Science)
3. author (if a PERSON wrote something)
4. worksAt (if a PERSON works at an ORGANIZATION)
5. locatedIn (if an ORGANIZATION or PERSON is in a PLACE/CITY)
6. collaboratesWith (if two PERSONS work together)
7. studiesAt (if a PERSON studies at an ORGANIZATION)
8. manages (if a PERSON manages an ORGANIZATION)
9. relatedTo (default fallback)

IMPORTANT:
- Use teachesSubject for academic subjects (Physics, Mathematics, Biology, etc.)
- Use teaches for teaching at a place/institution
- Only PERSONS can worksAt organizations
- Organizations are locatedIn places, NOT worksAt

Reply ONLY with the single word of the relation. No explanation."""

        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": "You are a Semantic Web expert. You output only JSON or single words."
                },
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama-3.1-8b-instant",  # Mod√®le gratuit actif et ultra-rapide
            temperature=0,  # Z√©ro cr√©ativit√© = r√©ponse stable
        )

        relation = chat_completion.choices[0].message.content.strip()
        
        # Nettoyage au cas o√π Llama est bavard
        relation = relation.lower().replace(".", "").replace('"', "").replace("'", "")
        
        # --- LOGIQUE DE CORRECTION S√âMANTIQUE AVEC PRIORIT√âS ET CONTEXTE LOCAL ---
        sentence_lower = sentence.lower()
        entity1_lower = entity1.lower()
        entity2_lower = entity2.lower()
        
        # Extraction du contexte local (15 mots autour des deux entit√©s)
        # Cela permet d'analyser uniquement le texte pertinent pour cette paire
        try:
            pos1 = sentence_lower.find(entity1_lower)
            pos2 = sentence_lower.find(entity2_lower)
            
            if pos1 >= 0 and pos2 >= 0:
                start = min(pos1, pos2)
                end = max(pos1 + len(entity1_lower), pos2 + len(entity2_lower))
                
                # Extraire 50 caract√®res avant et apr√®s pour le contexte
                context_start = max(0, start - 50)
                context_end = min(len(sentence_lower), end + 50)
                local_context = sentence_lower[context_start:context_end]
            else:
                local_context = sentence_lower  # Fallback sur phrase compl√®te
        except:
            local_context = sentence_lower
        
        print(f"    üîç Contexte local : ...{local_context}...")
        
        # Classification des lieux (villes vs b√¢timents/institutions)
        # IMPORTANT : Ne pas mettre de villes qui apparaissent souvent dans des noms d'institutions
        vraies_villes = ["paris", "france", "versailles", "lyon", "marseille", "toulouse", 
                        "bordeaux", "lille", "√©tats-unis", "usa", "new york", "londres",
                        "californie", "redmond", "cambridge", "oxford",
                        "berkeley", "boston", "seattle", "tokyo", "berlin"]
        
        batiments_institutions = ["palais", "√©lys√©e", "mit", "stanford", "harvard", "universit√©", 
                                 "university", "institut", "√©cole", "college"]
        
        # V√©rifier si entity2 est UNIQUEMENT une ville (pas dans un nom d'institution)
        is_vraie_ville = any(ville in entity2_lower for ville in vraies_villes) and \
                        not any(bat in entity2_lower for bat in batiments_institutions)
        is_batiment = any(bat in entity2_lower for bat in batiments_institutions)
        
        # === PRIORIT√âS BAS√âES SUR LE CONTEXTE LOCAL (pas toute la phrase) ===
        
        # PRIORIT√â 0 : Enseignement d'une mati√®re (si entity2 est un TOPIC)
        # D√©tection : "enseigne" + entity2 semble √™tre une mati√®re ou d√©tect√© comme TOPIC par Groq
        topics_keywords = ["physique", "math√©matiques", "maths", "informatique", "biologie", 
                          "chimie", "histoire", "g√©ographie", "philosophie", "litt√©rature",
                          "physics", "mathematics", "computer science", "biology", "chemistry",
                          "rdfs", "rdf", "owl", "sparql"]
        
        is_topic = any(kw in entity2_lower for kw in topics_keywords)
        
        if any(kw in local_context for kw in ["enseigne", "enseign√©", "enseignant", "teach", "taught", "teaching"]) and is_topic:
            relation = "teachesSubject"
            print(f"  üéì Priorit√© 0 : D√©tection 'enseigne' + mati√®re '{entity2}' ‚Üí Force teachesSubject (bypass validation)")
        
        # PRIORIT√â 1 : Enseignement (mots-cl√©s p√©dagogiques) - TOUJOURS EN PREMIER
        elif any(kw in local_context for kw in ["enseigne", "enseign√©", "enseignant", "professeur", "teach", "professor", "taught", "teaching"]):
            relation = "teaches"
            print(f"  üéì Priorit√© 1 : D√©tection 'enseigne/professeur' dans contexte local ‚Üí Force teaches")
        
        # PRIORIT√â 2 : Direction/Management (mots-cl√©s de management)
        # IMPORTANT : Seulement si entity1 est une personne ET entity2 est une organisation
        elif any(kw in local_context for kw in ["dirige", "g√®re", "manage", "manages", "ceo", "dirigeant"]) and \
             not is_vraie_ville:  # Exclure les villes (on ne dirige pas une ville)
            relation = "manages"
            print(f"  üíº Priorit√© 2 : D√©tection 'dirige/g√®re' dans contexte local ‚Üí Force manages")
        
        # PRIORIT√â 3 : Travail/Emploi (personne ‚Üí organisation/b√¢timent, PAS ville)
        elif any(kw in local_context for kw in ["travaille", "works", "employ√©", "employee"]):
            # Accepter worksAt seulement si entity2 n'est PAS une vraie ville
            if not is_vraie_ville or is_batiment:
                relation = "worksAt"
                print(f"  üíº Priorit√© 3 : D√©tection 'travaille' dans contexte local ‚Üí Force worksAt")
            else:
                relation = "locatedIn"
                print(f"  üìç 'travaille' + ville d√©tect√©e ‚Üí Force locatedIn")
        
        # PRIORIT√â 4 : R√©daction/Auteur (mots-cl√©s de cr√©ation)
        elif any(kw in local_context for kw in ["auteur", "r√©dig√©", "√©crit", "author", "wrote", "written", "√©crivain", "a √©crit"]):
            relation = "author"
            print(f"  ‚úçÔ∏è Priorit√© 4 : D√©tection 'auteur/√©crit' dans contexte local ‚Üí Force author")
        
        # PRIORIT√â 4.5 : Localisation explicite avec "situ√©" (prend le dessus sur manages)
        elif any(kw in local_context for kw in ["situ√©", "situ√©e", "bas√©", "bas√©e", "located", "based"]):
            if is_vraie_ville or is_batiment:
                relation = "locatedIn"
                print(f"  üìç Priorit√© 4.5 : D√©tection 'situ√©/bas√©' dans contexte local ‚Üí Force locatedIn")
        
        # PRIORIT√â 5 : Localisation (si entity2 est une vraie ville)
        elif is_vraie_ville:
            ville_detectee = next((v for v in vraies_villes if v in entity2_lower), entity2)
            print(f"  üìç Priorit√© 5 : D√©tection ville '{ville_detectee}' ‚Üí Force locatedIn")
            relation = "locatedIn"
        
        # Mapping de s√©curit√© pour les autres cas (d√©tection dans la r√©ponse LLM)
        # IMPORTANT : Ne pas toucher √† teachesSubject s'il a √©t√© forc√© en Priorit√© 0
        if relation != "teachesSubject":  # Protection contre √©crasement
            if "teachsubject" in relation.lower().replace(" ", "").replace("_", ""):
                relation = "teachesSubject"
            elif "teach" in relation:
                relation = "teaches"
            elif "author" in relation or "wrote" in relation:
                relation = "author"
            elif "work" in relation:
                relation = "worksAt"
            elif "located" in relation or "situ√©" in relation or "bas√©" in relation:
                relation = "locatedIn"
            elif "collabore" in relation or "collaborate" in relation:
                relation = "collaboratesWith"
            elif "√©tudie" in relation or "studies" in relation:
                relation = "studiesAt"
            elif "g√®re" in relation or "manage" in relation:
                relation = "manages"
        
        # Liste des relations valides
        valid_relations = ["teaches", "teachesSubject", "author", "worksAt", "locatedIn", 
                          "collaboratesWith", "studiesAt", "manages", "relatedTo"]
        if relation not in valid_relations:
            relation = "relatedTo"

        print(f"  ü§ñ Groq/Llama-3 a d√©tect√© : {entity1} --[{relation}]--> {entity2}")
        return relation

    except Exception as e:
        print(f"  ‚ö†Ô∏è Erreur Groq ({str(e)[:80]}). Passage au fallback.")
        # Fallback ultime avec d√©tection de lieux
        sentence_lower = sentence.lower()
        entity2_lower = entity2.lower()
        
        # D√©tection de lieux dans le fallback
        lieux = ["paris", "france", "versailles", "lyon", "marseille", "toulouse", 
                 "bordeaux", "lille", "√©tats-unis", "usa", "new york", "londres",
                 "californie", "silicon valley"]
        
        for lieu in lieux:
            if lieu in entity2_lower:
                print(f"  üîç Fallback : D√©tection de lieu '{lieu}' ‚Üí locatedIn")
                return "locatedIn"
        
        # Autres d√©tections
        if "enseigne" in sentence_lower or "teach" in sentence_lower:
            return "teaches"
        if "r√©dig√©" in sentence_lower or "√©crit" in sentence_lower or "author" in sentence_lower:
            return "author"
        if "travaille" in sentence_lower or "works" in sentence_lower:
            return "worksAt"
        if "situ√©" in sentence_lower or "bas√©" in sentence_lower or "located" in sentence_lower:
            return "locatedIn"
        if "collabore" in sentence_lower or "collaborate" in sentence_lower:
            return "collaboratesWith"
        if "√©tudie" in sentence_lower or "studies" in sentence_lower:
            return "studiesAt"
        
        return "relatedTo"


# ============================================================================
# 4.5. RAFFINEMENT INTELLIGENT DES TYPES D'ENTIT√âS VIA LLM
# ============================================================================

def refine_entity_types(entities, sentence):
    """
    Re-classifie dynamiquement les entit√©s d√©tect√©es par spaCy via Groq/Llama-3.
    
    Cette fonction permet de corriger les erreurs de spaCy et d'ajouter un nouveau type : TOPIC
    (pour les mati√®res acad√©miques, concepts scientifiques, domaines de connaissance).
    
    Args:
        entities (list): Liste de tuples (texte_entit√©, type_spacy)
        sentence (str): Phrase compl√®te pour le contexte
        
    Returns:
        list: Liste de tuples (texte_entit√©, type_raffin√©)
        
    Exemples:
        Input:  [("Einstein", "PER"), ("Physique", "MISC")]
        Output: [("Einstein", "PER"), ("Physique", "TOPIC")]
    """
    if not entities:
        return entities
    
    print("\n[RAFFINEMENT] Re-classification intelligente des entit√©s via Groq/Llama-3...")
    
    # Cl√© API Groq (charg√©e depuis .env)
    GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
    if not GROQ_API_KEY:
        print("‚ö†Ô∏è ATTENTION : GROQ_API_KEY non trouv√© dans .env")
        return entities  # Retourner entit√©s non raffin√©es
    
    try:
        client = Groq(api_key=GROQ_API_KEY)
        
        # Pr√©parer la liste des entit√©s pour le prompt
        entity_list = [text for text, _ in entities]
        entity_list_str = ", ".join([f'"{e}"' for e in entity_list])
        
        # Prompt optimis√© pour Llama-3
        prompt = f"""Context: "{sentence}"

Entities detected: [{entity_list_str}]

For each entity, determine its precise type from this list:
- PERSON: human being (name, pronoun)
- ORGANIZATION: company, institution, university, government body
- LOCATION: city, country, place, building, address
- TOPIC: academic subject, scientific field, concept, domain (e.g., Physics, Mathematics, Computer Science, Biology, RDFS, etc.)
- DOCUMENT: book title, article, publication, course name

Reply ONLY with a valid JSON object mapping each entity to its type.
Example: {{"Einstein": "PERSON", "Physique": "TOPIC", "Universit√© de Princeton": "ORGANIZATION"}}

JSON:"""

        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": "You are a semantic entity classifier. You output ONLY valid JSON."
                },
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama-3.1-8b-instant",
            temperature=0,
        )

        response = chat_completion.choices[0].message.content.strip()
        
        # Nettoyage : extraire le JSON (parfois Llama ajoute des backticks)
        if "```json" in response:
            response = response.split("```json")[1].split("```")[0].strip()
        elif "```" in response:
            response = response.split("```")[1].split("```")[0].strip()
        
        # Parser le JSON
        try:
            classification = json.loads(response)
        except json.JSONDecodeError:
            # Fallback : essayer de trouver un objet JSON dans la r√©ponse
            import re
            json_match = re.search(r'\{[^}]+\}', response)
            if json_match:
                classification = json.loads(json_match.group())
            else:
                print(f"  ‚ö†Ô∏è √âchec parsing JSON. R√©ponse brute: {response[:100]}")
                return entities
        
        # Appliquer la re-classification
        refined_entities = []
        for entity_text, original_type in entities:
            if entity_text in classification:
                new_type = classification[entity_text]
                
                # Mapper les types Groq vers les types de notre syst√®me
                type_mapping = {
                    "PERSON": "PER",
                    "ORGANIZATION": "ORG",
                    "LOCATION": "LOC",
                    "TOPIC": "TOPIC",  # Nouveau type !
                    "DOCUMENT": "DOC"
                }
                
                refined_type = type_mapping.get(new_type, original_type)
                
                if refined_type != original_type:
                    print(f"  üîÑ Raffinement : '{entity_text}' : {original_type} ‚Üí {refined_type}")
                else:
                    print(f"  ‚úì Confirm√© : '{entity_text}' : {refined_type}")
                
                refined_entities.append((entity_text, refined_type))
            else:
                # Entit√© non classifi√©e par Groq, garder le type original
                print(f"  ‚ÑπÔ∏è Non classifi√© : '{entity_text}' (conserv√©: {original_type})")
                refined_entities.append((entity_text, original_type))
        
        print(f"[RAFFINEMENT] ‚úì {len(refined_entities)} entit√©s re-classifi√©es\n")
        return refined_entities
        
    except Exception as e:
        print(f"  ‚ö†Ô∏è Erreur Groq lors du raffinement ({str(e)[:80]})")
        print(f"  ‚Üí Utilisation des types spaCy originaux")
        return entities


# ============================================================================
# 5. EXTRACTION DES ENTIT√âS (A-BOX) - DONN√âES FACTUELLES
# ============================================================================

def extract_entities_with_spacy(text, nlp):
    """
    Extrait les entit√©s nomm√©es d'un texte avec spaCy (reconnaissance NER).
    
    Cette fonction r√©alise la phase d'extraction factuelle qui alimentera
    la A-Box (Assertional Box) avec des instances concr√®tes.
    
    Args:
        text (str): Le texte source √† analyser
        nlp (spacy.Language): Mod√®le spaCy charg√©
        
    Returns:
        list: Liste de tuples (texte_entit√©, type_entit√©)
              Exemple: [("Zoubida Kedad", "PER"), ("Universit√© de Versailles", "LOC")]
    """
    print("\n[A-BOX] Extraction des entit√©s nomm√©es avec spaCy...")
    doc = nlp(text)
    
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
        print(f"  ‚úì Entit√© d√©tect√©e : '{ent.text}' ‚Üí Type : {ent.label_}")
    
    return entities


def instantiate_entities_in_abox(graph, entities):
    """
    Instancie les entit√©s extraites dans le graphe RDF (A-Box).
    
    Pour chaque entit√© :
    1. Cr√©e une URI unique dans le namespace DATA
    2. Assigne le type de classe appropri√© (rdf:type) selon le mapping :
       - PER (Personne) ‚Üí ex:Person
       - LOC (Lieu) ‚Üí ex:Location
       - ORG (Organisation) ‚Üí ex:Organization
    3. Ajoute une propri√©t√© de donn√©es (owl:DatatypeProperty) pour le nom
    
    Args:
        graph (rdflib.Graph): Le graphe RDF o√π ajouter les instances
        entities (list): Liste de tuples (texte, type) issus de spaCy
        
    Returns:
        dict: Mapping {texte_entit√©: URI} pour r√©f√©rencer les instances
    """
    print("\n[A-BOX] Instanciation des entit√©s dans le graphe...")
    
    # Mapping entre les types NER (spaCy + Groq raffin√©s) et les classes OWL standards
    entity_type_mapping = {
        "PER": FOAF.Person,           # Personne ‚Üí foaf:Person (standard FOAF)
        "LOC": SCHEMA.Place,          # Lieu ‚Üí schema:Place (standard Schema.org)
        "ORG": SCHEMA.Organization,   # Organisation ‚Üí schema:Organization (standard Schema.org)
        "TOPIC": EX.Document,         # ‚ú® NOUVEAU : Mati√®re/Concept ‚Üí ex:Document (sujet d'√©tude)
        "DOC": EX.Document            # Document explicite
    }
    
    entity_uris = {}
    
    for entity_text, entity_label in entities:
        # NETTOYAGE URI : Transformation en snake_case sans accents
        uri_fragment = normalize_uri_fragment(entity_text)
        entity_uri = DATA[uri_fragment]
        
        # Gestion du nouveau type TOPIC (mati√®res acad√©miques, concepts scientifiques)
        if entity_label == "TOPIC":
            print(f"  üìö Entit√© TOPIC d√©tect√©e (mati√®re/concept) : '{entity_text}'")
            graph.add((entity_uri, RDF.type, EX.Document))
            graph.add((entity_uri, RDFS.label, Literal(entity_text, lang="fr")))
            graph.add((entity_uri, FOAF.name, Literal(entity_text, lang="fr")))
            entity_uris[entity_text] = entity_uri
            print(f"  ‚úì Instance cr√©√©e : {uri_fragment} (type: Topic/Document, label: '{entity_text}')")
            continue
        
        # Gestion intelligente des entit√©s MISC (≈ìuvres, documents, concepts)
        if entity_label == "MISC":
            # Mots-cl√©s indiquant un document/≈ìuvre
            document_keywords = ["roman", "livre", "cours", "sp√©cifications", "document", 
                               "article", "publication", "ouvrage", "≈ìuvre", "the", "les", "le"]
            is_document = any(kw.lower() in entity_text.lower() for kw in document_keywords)
            
            if is_document:
                print(f"  üìö Entit√© MISC d√©tect√©e comme Document : '{entity_text}'")
                graph.add((entity_uri, RDF.type, EX.Document))
                graph.add((entity_uri, RDFS.label, Literal(entity_text, lang="fr")))
                graph.add((entity_uri, FOAF.name, Literal(entity_text, lang="fr")))
                entity_uris[entity_text] = entity_uri
                print(f"  ‚úì Instance cr√©√©e : {uri_fragment} (type: Document, label: '{entity_text}')")
                continue
            else:
                print(f"  ‚ö† Type d'entit√© MISC non reconnu comme document : {entity_text} (ignor√©)")
                continue
        
        # V√©rification que le type d'entit√© est reconnu
        if entity_label not in entity_type_mapping:
            print(f"  ‚ö† Type d'entit√© non mapp√© : {entity_label} (ignor√©)")
            continue
        
        # TYPAGE STRICT : Assignation du type de classe selon Spacy NER
        # PER ‚Üí foaf:Person | ORG ‚Üí schema:Organization | LOC ‚Üí schema:Place
        owl_class = entity_type_mapping[entity_label]
        graph.add((entity_uri, RDF.type, owl_class))
        
        # LABELS SYST√âMATIQUES : Ajout de rdfs:label et foaf:name
        # rdfs:label = √©tiquette RDF standard (pour raisonneurs)
        graph.add((entity_uri, RDFS.label, Literal(entity_text, lang="fr")))
        
        # foaf:name = nom lisible (standard FOAF)
        graph.add((entity_uri, FOAF.name, Literal(entity_text, lang="fr")))
        
        entity_uris[entity_text] = entity_uri
        type_display = owl_class.split('#')[-1].split('/')[-1]
        print(f"  ‚úì Instance cr√©√©e : {uri_fragment} (type: {type_display}, label: '{entity_text}')")
    
    return entity_uris


# ============================================================================
# 5. RELATIONS S√âMANTIQUES (ObjectProperties dans l'A-Box)
# ============================================================================

def adapt_entity_type(graph, entity_uri, entity_text, required_type):
    """
    Adapte dynamiquement le type d'une entit√© selon le contexte de la relation.
    
    Un lieu peut √™tre une organisation (ex: Palais de l'√âlys√©e = lieu de travail).
    Cette fonction ajoute un type suppl√©mentaire sans supprimer le type original.
    
    Args:
        graph: Le graphe RDF
        entity_uri: L'URI de l'entit√© √† adapter
        entity_text: Le texte de l'entit√© (pour logs)
        required_type: Le type requis par la relation
    """
    current_types = list(graph.objects(entity_uri, RDF.type))
    
    # Si l'entit√© est un Place et qu'on a besoin d'Organization ‚Üí ajouter Organization
    if SCHEMA.Place in current_types and required_type == SCHEMA.Organization:
        graph.add((entity_uri, RDF.type, SCHEMA.Organization))
        print(f"    üîÑ Typage adaptatif : {entity_text} est aussi une Organisation (contexte professionnel)")
        return True
    
    # Si l'entit√© est un Place et qu'on a besoin de Document ‚Üí possible pour institutions
    if SCHEMA.Place in current_types and required_type == EX.Document:
        graph.add((entity_uri, RDF.type, EX.Document))
        print(f"    üîÑ Typage adaptatif : {entity_text} est aussi un Document")
        return True
    
    return False


def extract_relations(graph, entity_uris, text):
    """
    Extrait et instancie les relations s√©mantiques entre entit√©s.
    
    Cette fonction utilise la fonction predict_relation() qui simule un LLM
    pour d√©tecter automatiquement les relations entre entit√©s.
    
    Am√©liorations par rapport √† la version pr√©c√©dente :
    - Utilisation d'un "LLM Mock" via predict_relation() (extensible vers vraie API)
    - D√©tection automatique du type de relation
    - Support de multiples types de relations
    - Typage adaptatif : un lieu peut √™tre une organisation selon le contexte
    
    Args:
        graph (rdflib.Graph): Le graphe RDF o√π ajouter les relations
        entity_uris (dict): Mapping des entit√©s vers leurs URIs
        text (str): Le texte source pour l'analyse contextuelle
    """
    print("\n[A-BOX] Extraction des relations s√©mantiques avec LLM Mock...")
    
    # Parcours de toutes les paires d'entit√©s possibles
    entities_list = list(entity_uris.items())
    
    for i, (entity1_text, entity1_uri) in enumerate(entities_list):
        for j, (entity2_text, entity2_uri) in enumerate(entities_list):
            if i >= j:  # √âviter les doublons et auto-relations
                continue
            
            # Utilisation de l'API Hugging Face R√âELLE pour pr√©dire la relation ‚≠ê
            relation_type = predict_relation_real_api(entity1_text, entity2_text, text)
            
            if relation_type is None:
                continue
            
            # Mapping des relations pr√©dites vers les propri√©t√©s OWL avec contraintes flexibles
            # Note : teaches accepte Place OU Organization (universit√© = organisation)
            relation_mapping = {
                "teaches": (EX.teaches, FOAF.Person, [SCHEMA.Place, SCHEMA.Organization]),  # Personne ‚Üí Lieu OU Organisation
                "teachesSubject": (EX.teachesSubject, FOAF.Person, EX.Document),  # ‚ú® Personne ‚Üí Mati√®re/Topic
                "author": (EX.author, FOAF.Person, EX.Document),  # Auteur ‚Üí Document
                "worksAt": (EX.worksAt, FOAF.Person, SCHEMA.Organization),
                "locatedIn": (EX.locatedIn, None, SCHEMA.Place),  # Organisation/Personne ‚Üí Lieu
                "collaboratesWith": (EX.collaboratesWith, FOAF.Person, FOAF.Person),
                "studiesAt": (EX.studiesAt, FOAF.Person, SCHEMA.Organization),
                "manages": (EX.manages, FOAF.Person, SCHEMA.Organization),
                "relatedTo": (EX.relatedTo, None, None)
            }
            
            if relation_type not in relation_mapping:
                continue
            
            relation_prop, expected_domain, expected_range = relation_mapping[relation_type]
            
            # BYPASS SP√âCIAL : Si teachesSubject d√©tect√© en Priorit√© 0, ajouter directement
            # (car entity2 est un TOPIC d√©tect√© par Groq, pas encore typ√© dans le graphe)
            if relation_type == "teachesSubject":
                # V√©rifier que entity1 est bien une Person
                domain_valid = (entity1_uri, RDF.type, FOAF.Person) in graph
                # V√©rifier que entity2 est un Document/TOPIC
                range_valid = (entity2_uri, RDF.type, EX.Document) in graph
                
                if domain_valid and range_valid:
                    graph.add((entity1_uri, relation_prop, entity2_uri))
                    print(f"  ‚úì Relation LLM : {entity1_text} --[{relation_type}]--> {entity2_text}")
                else:
                    if not domain_valid:
                        print(f"  ‚ö†Ô∏è teachesSubject rejet√© : {entity1_text} n'est pas une Person")
                    if not range_valid:
                        print(f"  ‚ö†Ô∏è teachesSubject rejet√© : {entity2_text} n'est pas un Document/Topic")
                continue
            
            # VALIDATION FLEXIBLE AVEC TYPAGE ADAPTATIF ET MULTIPLES TYPES ACCEPT√âS
            if expected_domain and expected_range:
                # V√©rifier que les types correspondent aux contraintes
                domain_valid = (entity1_uri, RDF.type, expected_domain) in graph
                
                # Si expected_range est une liste, accepter n'importe quel type de la liste
                if isinstance(expected_range, list):
                    range_valid = any((entity2_uri, RDF.type, rtype) in graph for rtype in expected_range)
                else:
                    range_valid = (entity2_uri, RDF.type, expected_range) in graph
                
                # Si le range n'est pas valide, tenter un typage adaptatif
                if not range_valid and expected_range:
                    if isinstance(expected_range, list):
                        # Essayer d'adapter au premier type de la liste
                        range_valid = adapt_entity_type(graph, entity2_uri, entity2_text, expected_range[0])
                    else:
                        range_valid = adapt_entity_type(graph, entity2_uri, entity2_text, expected_range)
                
                # Si le domain n'est pas valide, tenter un typage adaptatif
                if not domain_valid and expected_domain:
                    domain_valid = adapt_entity_type(graph, entity1_uri, entity1_text, expected_domain)
                
                if domain_valid and range_valid:
                    graph.add((entity1_uri, relation_prop, entity2_uri))
                    print(f"  ‚úì Relation LLM : {entity1_text} --[{relation_type}]--> {entity2_text}")
                    
                    # RESTRICTION OWL : Si c'est une relation 'author', typer le document en ValidatedCourse
                    if relation_type == "author":
                        # Ajouter le type ValidatedCourse pour valider la contrainte OWL
                        graph.add((entity2_uri, RDF.type, EX.ValidatedCourse))
                        print(f"    ‚Üí Contrainte OWL : {entity2_text} typ√© en ValidatedCourse")
                else:
                    if not domain_valid:
                        print(f"  ‚ö†Ô∏è Type domain invalide pour {entity1_text} (attendu: {expected_domain})")
                    if not range_valid:
                        range_str = ', '.join([str(r) for r in expected_range]) if isinstance(expected_range, list) else str(expected_range)
                        print(f"  ‚ö†Ô∏è Type range invalide pour {entity2_text} (attendu: {range_str})")
                        
            elif expected_domain is None and expected_range:
                # Seulement le range est sp√©cifi√© (ex: locatedIn ‚Üí Place)
                if (entity2_uri, RDF.type, expected_range) in graph:
                    graph.add((entity1_uri, relation_prop, entity2_uri))
                    print(f"  ‚úì Relation LLM : {entity1_text} --[{relation_type}]--> {entity2_text}")
                else:
                    print(f"  ‚ö†Ô∏è Type range invalide pour {entity2_text} (attendu: Place)")
                    
            elif relation_type == "relatedTo":
                # Relation g√©n√©rique sans contrainte de type
                graph.add((entity1_uri, relation_prop, entity2_uri))
                print(f"  ‚úì Relation LLM : {entity1_text} --[{relation_type}]--> {entity2_text}")


# ============================================================================
# 6. R√âIFICATION RDF - M√âTADONN√âES SUR LES TRIPLETS
# ============================================================================

def reify_statement(graph, subject, predicate, obj, source_file="texte_exemple.txt"):
    """
    Applique la r√©ification RDF pour attacher des m√©tadonn√©es √† un triplet.
    
    Concept th√©orique (CRUCIAL pour le cours) :
    --------------------------------------------
    La r√©ification permet de "faire une assertion sur une assertion".
    Au lieu de simplement dire :
        <Sujet> <Pr√©dicat> <Objet>
    
    On cr√©e un n≈ìud interm√©diaire (rdf:Statement) qui repr√©sente le triplet lui-m√™me,
    puis on peut ajouter des m√©tadonn√©es √† ce n≈ìud :
        _:statement1 rdf:type rdf:Statement
        _:statement1 rdf:subject <Sujet>
        _:statement1 rdf:predicate <Pr√©dicat>
        _:statement1 rdf:object <Objet>
        _:statement1 dc:source "fichier.txt"
        _:statement1 dc:date "2026-01-15"
    
    Cas d'usage :
    - Tra√ßabilit√© de la source d'information
    - Confiance/Score de certitude d'une extraction
    - Horodatage d'une assertion
    - Provenance des donn√©es
    
    Args:
        graph (rdflib.Graph): Le graphe RDF
        subject (URIRef): Le sujet du triplet √† r√©ifier
        predicate (URIRef): Le pr√©dicat du triplet
        obj (URIRef or Literal): L'objet du triplet
        source_file (str): Nom du fichier source (m√©tadonn√©e)
        
    Returns:
        URIRef: L'URI du n≈ìud de r√©ification cr√©√©
    """
    # Cr√©ation d'un n≈ìud unique pour repr√©senter le triplet
    # Ici on utilise un BNode (Blank Node) mais on pourrait aussi utiliser une URI nomm√©e
    statement_uri = DATA[f"statement_{hash((subject, predicate, obj)) & 0xFFFFFF}"]
    
    # D√©claration du type : c'est un rdf:Statement (n≈ìud de r√©ification)
    graph.add((statement_uri, RDF.type, RDF.Statement))
    
    # D√©composition du triplet en propri√©t√©s RDF standard
    graph.add((statement_uri, RDF.subject, subject))    # Qui ?
    graph.add((statement_uri, RDF.predicate, predicate)) # Fait quoi ?
    graph.add((statement_uri, RDF.object, obj))          # √Ä qui/quoi ?
    
    # Ajout de m√©tadonn√©es sur le triplet
    # Ici : la source d'extraction (Dublin Core Metadata)
    graph.add((statement_uri, DC.source, Literal(source_file, datatype=XSD.string)))
    
    # On pourrait ajouter d'autres m√©tadonn√©es :
    # graph.add((statement_uri, DC.date, Literal("2026-01-15", datatype=XSD.date)))
    # graph.add((statement_uri, EX.confidence, Literal(0.95, datatype=XSD.float)))
    
    print(f"  ‚úì R√©ification cr√©√©e pour : {subject.split('#')[-1]} --{predicate.split('#')[-1]}--> ...")
    
    return statement_uri


def apply_reification_to_relations(graph, source_file="texte_exemple.txt"):
    """
    Parcourt toutes les relations ObjectProperty du graphe et applique la r√©ification.
    
    Cette fonction d√©montre l'application syst√©matique de la r√©ification
    sur les assertions extraites, permettant la tra√ßabilit√© compl√®te.
    
    Args:
        graph (rdflib.Graph): Le graphe RDF contenant les assertions
        source_file (str): Nom du fichier source
    """
    print("\n[R√âIFICATION] Application de la r√©ification aux relations...")
    
    # Liste des propri√©t√©s ObjectProperty √† r√©ifier
    properties_to_reify = [EX.teaches, EX.author, EX.traite_de, EX.relatedTo, EX.worksAt]
    
    reified_count = 0
    for prop in properties_to_reify:
        # Parcours de tous les triplets utilisant cette propri√©t√©
        for subject, predicate, obj in graph.triples((None, prop, None)):
            reify_statement(graph, subject, predicate, obj, source_file)
            reified_count += 1
    
    print(f"[R√âIFICATION] {reified_count} triplet(s) r√©ifi√©(s) avec m√©tadonn√©es dc:source\n")


# ============================================================================
# 7. VISUALISATION GRAPHIQUE DU GRAPHE DE CONNAISSANCES
# ============================================================================

def visualize_knowledge_graph(graph, output_file="graphe_connaissance.png"):
    """
    G√©n√®re une visualisation graphique du graphe de connaissances RDF.
    
    Cette fonction cr√©e une repr√©sentation visuelle interactive du graphe,
    utile pour la d√©monstration et la compr√©hension des relations extraites.
    
    Fonctionnalit√©s :
    - Utilisation de NetworkX pour la structure du graphe
    - Colorisation par type de n≈ìud
    - √âtiquettes des relations sur les ar√™tes
    - Export en format PNG
    
    Args:
        graph (rdflib.Graph): Le graphe RDF √† visualiser
        output_file (str): Chemin du fichier de sortie (PNG)
    """
    print("\n[VISUALISATION] G√©n√©ration du graphe visuel...")
    
    # Cr√©ation d'un graphe NetworkX dirig√©
    G = nx.DiGraph()
    
    # Dictionnaires pour stocker les m√©tadonn√©es des n≈ìuds
    node_types = {}  # URI -> type (Person, Place, etc.)
    node_labels = {} # URI -> label lisible
    edge_labels = {} # (source, target) -> nom de la relation
    
    # Extraction des n≈ìuds (entit√©s) et leurs types
    for subject, predicate, obj in graph:
        # FILTRE 1 : Ignorer les BNodes (n≈ìuds anonymes comme les restrictions OWL)
        if isinstance(subject, BNode) or isinstance(obj, BNode):
            continue
        
        # FILTRE 2 : Ignorer les triplets de d√©finition d'ontologie (T-Box)
        ontology_predicates = [RDF.type, RDFS.domain, RDFS.range, RDFS.label, RDFS.comment, 
                              RDFS.subClassOf, OWL.onProperty, OWL.someValuesFrom, OWL.Restriction]
        
        if predicate in ontology_predicates:
            # Exception : Capturer le type des entit√©s (PER/ORG/LOC)
            if predicate == RDF.type and obj in [FOAF.Person, SCHEMA.Place, SCHEMA.Organization, EX.Document, EX.ValidatedCourse]:
                node_types[str(subject)] = str(obj).split('#')[-1].split('/')[-1]
            # Exception : Capturer les labels
            if predicate == RDFS.label:
                node_labels[str(subject)] = str(obj)
            continue
        
        # FILTRE 3 : Ignorer COMPL√àTEMENT les triplets de r√©ification
        # - Statement nodes (ex: data:statement_2460565)
        # - Pr√©dicats de r√©ification (rdf:subject, rdf:predicate, rdf:object, dc:source)
        if (str(subject).startswith(str(DATA) + 'statement_') or 
            obj == RDF.Statement or 
            predicate in [RDF.subject, RDF.predicate, RDF.object, DC.source]):
            continue
        
        # FILTRE 4 : Capturer foaf:name pour les labels (mais ne pas l'afficher comme ar√™te)
        if predicate == FOAF.name:
            if str(subject) not in node_labels:
                node_labels[str(subject)] = str(obj)[:30]
            continue
        
        # FILTRE 5 : Ignorer les autres DatatypeProperty (√¢ge, intitul√©)
        if predicate in [EX.intitule, EX.age]:
            continue
        
        # AJOUT AU GRAPHE : Ajouter uniquement les relations ObjectProperty entre entit√©s r√©elles
        if isinstance(obj, URIRef):
            # V√©rifier que ce n'est pas une classe d'ontologie
            if obj not in [OWL.Class, OWL.ObjectProperty, OWL.DatatypeProperty, OWL.Restriction]:
                G.add_edge(str(subject), str(obj))
                
                # Extraire le nom de la relation
                relation_name = str(predicate).split('#')[-1].split('/')[-1]
                edge_labels[(str(subject), str(obj))] = relation_name
    
    if len(G.nodes()) == 0:
        print("  ‚ö† Aucun n≈ìud √† visualiser (graphe vide)")
        return
    
    print(f"  ‚úì Graphe NetworkX cr√©√© : {len(G.nodes())} n≈ìuds, {len(G.edges())} ar√™tes")
    print(f"  ‚úì Entit√©s d√©tect√©es : {', '.join([node_labels.get(n, n.split('#')[-1]) for n in list(G.nodes())[:5]])}")
    
    # Pr√©paration de la figure
    plt.figure(figsize=(14, 10))
    plt.title("Graphe de Connaissances Extrait\n(T-Box + A-Box avec R√©ification)", 
              fontsize=16, fontweight='bold')
    
    # Layout du graphe (positionnement des n≈ìuds)
    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
    
    # Colorisation par type de n≈ìud
    color_map = {
        'Person': '#FF6B6B',      # Rouge pour les personnes
        'Place': '#4ECDC4',       # Turquoise pour les lieux
        'Organization': '#45B7D1', # Bleu pour les organisations
        'Document': '#FFA07A',     # Orange pour les documents
        'default': '#95E1D3'       # Vert par d√©faut
    }
    
    node_colors = []
    for node in G.nodes():
        node_type = node_types.get(node, 'default')
        node_colors.append(color_map.get(node_type, color_map['default']))
    
    # Dessiner les n≈ìuds
    nx.draw_networkx_nodes(G, pos, 
                          node_color=node_colors,
                          node_size=3000,
                          alpha=0.9,
                          edgecolors='black',
                          linewidths=2)
    
    # Dessiner les ar√™tes (relations)
    nx.draw_networkx_edges(G, pos,
                          edge_color='#333333',
                          arrows=True,
                          arrowsize=20,
                          arrowstyle='->',
                          width=2,
                          alpha=0.6,
                          connectionstyle='arc3,rad=0.1')
    
    # Pr√©parer les labels des n≈ìuds (noms courts)
    display_labels = {}
    for node in G.nodes():
        if node in node_labels:
            label = node_labels[node]
        else:
            # Extraire le dernier segment de l'URI
            label = node.split('#')[-1].split('/')[-1]
            label = label.replace('_', ' ').title()
        
        # Limiter la longueur pour la lisibilit√©
        if len(label) > 20:
            label = label[:17] + "..."
        display_labels[node] = label
    
    # Dessiner les labels des n≈ìuds
    nx.draw_networkx_labels(G, pos,
                           labels=display_labels,
                           font_size=9,
                           font_weight='bold',
                           font_color='white',
                           bbox=dict(boxstyle='round,pad=0.3', 
                                    facecolor='black', 
                                    alpha=0.7))
    
    # Dessiner les labels des ar√™tes (relations)
    nx.draw_networkx_edge_labels(G, pos,
                                edge_labels=edge_labels,
                                font_size=8,
                                font_color='#D32F2F',
                                bbox=dict(boxstyle='round,pad=0.3',
                                         facecolor='yellow',
                                         alpha=0.7))
    
    # L√©gende
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor=color_map['Person'], edgecolor='black', label='Personne (FOAF)'),
        Patch(facecolor=color_map['Place'], edgecolor='black', label='Lieu (Schema.org)'),
        Patch(facecolor=color_map['Organization'], edgecolor='black', label='Organisation (Schema.org)'),
        Patch(facecolor=color_map['Document'], edgecolor='black', label='Document')
    ]
    plt.legend(handles=legend_elements, loc='upper left', fontsize=10)
    
    plt.axis('off')
    plt.tight_layout()
    
    # Sauvegarde
    plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"  ‚úì Graphe sauvegard√© : {output_file}")
    print(f"  ‚úì N≈ìuds : {len(G.nodes())}, Ar√™tes : {len(G.edges())}")
    
    # Fermeture pour lib√©rer la m√©moire
    plt.close()


# ============================================================================
# 8. FONCTION PRINCIPALE - ORCHESTRATION DU PIPELINE
# ============================================================================

def main():
    """
    Fonction principale orchestrant l'ensemble du pipeline d'extraction
    de graphe de connaissances selon l'architecture T-Box/A-Box.
    
    IMPORTANT : Chaque ex√©cution cr√©e un NOUVEAU graphe vide pour √©viter
    la pollution de donn√©es entre extractions successives.
    """
    print("="*80)
    print("PROJET MASTER 2 - WEB S√âMANTIQUE")
    print("Extraction de Graphe de Connaissances avec Architecture T-Box/A-Box")
    print("="*80)
    
    # -----------------------------------------------------------------------
    # NETTOYAGE : Supprimer les anciens fichiers pour √©viter la pollution
    # -----------------------------------------------------------------------
    for old_file in ["knowledge_graph.ttl", "knowledge_graph.xml", "graphe_connaissance.png"]:
        if os.path.exists(old_file):
            os.remove(old_file)
            print(f"[CLEANUP] Ancien fichier supprim√© : {old_file}")
    
    # -----------------------------------------------------------------------
    # LECTURE DU TEXTE SOURCE (depuis fichier temporaire ou argument)
    # -----------------------------------------------------------------------
    text_example = "Zoubida Kedad enseigne √† l'Universit√© de Versailles. Elle a r√©dig√© un cours sur RDFS."
    
    # Option 1 : Lire depuis texte_temp.txt (utilis√© par Streamlit)
    if os.path.exists("texte_temp.txt"):
        try:
            with open("texte_temp.txt", "r", encoding="utf-8") as f:
                custom_text = f.read().strip()
                if custom_text:
                    text_example = custom_text
                    print(f"[INFO] Texte charg√© depuis texte_temp.txt\n")
        except Exception as e:
            print(f"[WARNING] Erreur lecture texte_temp.txt : {e}")
    
    # Option 2 : Lire depuis argument en ligne de commande
    if len(sys.argv) > 1:
        if sys.argv[1] == "--text" and len(sys.argv) > 2:
            text_example = sys.argv[2]
            print(f"[INFO] Texte charg√© depuis argument --text\n")
        elif sys.argv[1] != "--text":
            # Tout le reste est consid√©r√© comme le texte
            text_example = " ".join(sys.argv[1:])
            print(f"[INFO] Texte charg√© depuis arguments\n")
    
    # -----------------------------------------------------------------------
    # INITIALISATION : Cr√©ation d'un NOUVEAU graphe RDF vide
    # -----------------------------------------------------------------------
    # IMPORTANT : Le graphe est cr√©√© LOCALEMENT dans main() √† chaque ex√©cution
    # Cela garantit qu'aucune donn√©e r√©siduelle n'est conserv√©e entre les runs
    print("[INIT] Cr√©ation d'un nouveau graphe RDF vide...")
    graph = Graph()
    graph.bind("ex", EX)        # Notre ontologie
    graph.bind("data", DATA)    # Nos instances
    graph.bind("foaf", FOAF)    # FOAF (Friend of a Friend)
    graph.bind("schema", SCHEMA) # Schema.org
    graph.bind("owl", OWL)      # OWL (Web Ontology Language)
    graph.bind("rdf", RDF)      # RDF
    graph.bind("rdfs", RDFS)    # RDFS (vocabulaire de sch√©ma)
    graph.bind("xsd", XSD)      # Types de donn√©es XML Schema
    graph.bind("dc", DC)        # Dublin Core (m√©tadonn√©es)
    print("  ‚úì Graphe initialis√© avec namespaces\n")
    
    # -----------------------------------------------------------------------
    # PHASE 1 : D√©finition de l'ontologie (T-Box)
    # -----------------------------------------------------------------------
    define_tbox(graph)
    
    # -----------------------------------------------------------------------
    # PHASE 2 : Chargement du mod√®le NLP fran√ßais
    # -----------------------------------------------------------------------
    print("[NLP] Chargement du mod√®le spaCy fran√ßais...")
    try:
        nlp = spacy.load("fr_core_news_sm")
        print("  ‚úì Mod√®le 'fr_core_news_sm' charg√© avec succ√®s\n")
    except OSError:
        print("  ‚ö† Mod√®le non trouv√©. Installation automatique...")
        print("  Ex√©cutez : python -m spacy download fr_core_news_sm")
        print("  Utilisation d'un mod√®le de secours...\n")
        # Pour la d√©mo, on pourrait continuer avec des entit√©s hardcod√©es
        # mais id√©alement on devrait installer le mod√®le
        return
    
    # -----------------------------------------------------------------------
    # PHASE 3 : Extraction des entit√©s (A-Box - partie 1)
    # -----------------------------------------------------------------------
    print(f"[TEXTE SOURCE] : \"{text_example}\"\n")
    
    entities = extract_entities_with_spacy(text_example, nlp)
    
    # -----------------------------------------------------------------------
    # PHASE 3.5 : Raffinement intelligent des types via Groq/Llama-3 ‚ú®
    # -----------------------------------------------------------------------
    # Re-classification dynamique pour d√©tecter les TOPICS (mati√®res, concepts)
    # et corriger les erreurs de spaCy
    entities = refine_entity_types(entities, text_example)
    
    entity_uris = instantiate_entities_in_abox(graph, entities)
    
    # -----------------------------------------------------------------------
    # PHASE 4 : Extraction des relations (A-Box - partie 2)
    # -----------------------------------------------------------------------
    extract_relations(graph, entity_uris, text_example)
    
    # -----------------------------------------------------------------------
    # PHASE 5 : R√©ification des triplets
    # -----------------------------------------------------------------------
    apply_reification_to_relations(graph, source_file="texte_exemple.txt")
    
    # -----------------------------------------------------------------------
    # PHASE 6 : S√©rialisation et export (DOUBLE FORMAT : TURTLE + XML)
    # -----------------------------------------------------------------------
    print("="*80)
    print("[EXPORT] S√©rialisation du graphe RDF en deux formats")
    print("="*80)
    
    # FORMAT 1 : TURTLE (lisible par l'humain)
    output_file_turtle = "knowledge_graph.ttl"
    turtle_output = graph.serialize(format='turtle')
    
    with open(output_file_turtle, 'w', encoding='utf-8') as f:
        f.write(turtle_output)
    
    print(f"\n‚úì Graphe export√© en TURTLE : {output_file_turtle}")
    
    # FORMAT 2 : RDF/XML (standard historique du W3C, utilis√© dans le cours)
    output_file_xml = "knowledge_graph.xml"
    xml_output = graph.serialize(format='xml')
    
    with open(output_file_xml, 'w', encoding='utf-8') as f:
        f.write(xml_output)
    
    print(f"‚úì Graphe export√© en RDF/XML : {output_file_xml}")
    print(f"‚úì Nombre total de triplets : {len(graph)}")
    
    # -----------------------------------------------------------------------
    # PHASE 7 : Visualisation graphique
    # -----------------------------------------------------------------------
    visualize_knowledge_graph(graph, "graphe_connaissance.png")
    
    print("\n" + "="*80)
    print("R√âSULTAT FINAL - FORMAT TURTLE")
    print("="*80 + "\n")
    print(turtle_output)
    
    # -----------------------------------------------------------------------
    # Statistiques finales
    # -----------------------------------------------------------------------
    print("\n" + "="*80)
    print("STATISTIQUES DU GRAPHE")
    print("="*80)
    print(f"Classes (owl:Class) : {len(list(graph.subjects(RDF.type, OWL.Class)))}")
    print(f"  ‚Üí Dont classes avec restrictions OWL : 1 (ex:ValidatedCourse)")
    print(f"ObjectProperties : {len(list(graph.subjects(RDF.type, OWL.ObjectProperty)))}")
    print(f"DatatypeProperties : {len(list(graph.subjects(RDF.type, OWL.DatatypeProperty)))}")
    print(f"Restrictions OWL : {len(list(graph.subjects(RDF.type, OWL.Restriction)))}")
    print(f"Instances extraites : {len(entity_uris)}")
    print(f"Triplets r√©ifi√©s : {len(list(graph.subjects(RDF.type, RDF.Statement)))}")
    print(f"Total de triplets RDF : {len(graph)}")
    print("="*80 + "\n")
    
    print("‚úì Pipeline termin√© avec succ√®s !")
    print("‚úì Votre graphe de connaissances respecte les standards OWL et RDFS")
    print("‚úì Architecture T-Box/A-Box clairement s√©par√©e")
    print("‚úì Restrictions OWL appliqu√©es (ex:ValidatedCourse)")
    print("‚úì R√©ification appliqu√©e pour la tra√ßabilit√©")
    print("‚úì Extraction de relations via API Hugging Face R√âELLE (Mistral-7B) ‚≠ê")
    print("‚úì Double s√©rialisation : Turtle + RDF/XML\n")


# ============================================================================
# POINT D'ENTR√âE
# ============================================================================

if __name__ == "__main__":
    main()
